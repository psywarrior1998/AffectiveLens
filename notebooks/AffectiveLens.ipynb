{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initialized tokenizer and model: distilbert-base-uncased on cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Core Imports, Global Configuration, and Initializations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "# Hugging Face specific imports\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Device and Model Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "# --- CRITICAL: Initialize Tokenizer and Model (Done ONCE) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "model.eval()  # Set to evaluation mode for inference\n",
    "model.to(device)  # Move model to the configured device\n",
    "print(f\"Initialized tokenizer and model: {model_checkpoint} on {device}\")\n",
    "\n",
    "# --- Label and Path Definitions ---\n",
    "goemotions_labels = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\",\n",
    "    \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\",\n",
    "    \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "BROAD_EMOTION_CATEGORIES = ['negative', 'neutral', 'positive']\n",
    "NUM_BROAD_LABELS = len(BROAD_EMOTION_CATEGORIES)\n",
    "BROAD_EMOTION_CATEGORIES_MAPPING = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "positive_indices = {i for i, label in enumerate(goemotions_labels) if label in [\"admiration\", \"amusement\", \"approval\", \"caring\", \"curiosity\", \"desire\", \"excitement\", \"gratitude\", \"joy\", \"love\", \"optimism\", \"pride\", \"relief\", \"surprise\"]}\n",
    "negative_indices = {i for i, label in enumerate(goemotions_labels) if label in [\"anger\", \"annoyance\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"fear\", \"grief\", \"nervousness\", \"remorse\", \"sadness\"]}\n",
    "neutral_indices = {i for i, label in enumerate(goemotions_labels) if label in [\"neutral\", \"confusion\", \"realization\"]}\n",
    "\n",
    "# Define paths for the final embedding checkpoints\n",
    "EMBEDDINGS_TRAIN_POOL_PATH = './data/embeddings/MentalTrain'\n",
    "EMBEDDINGS_TEST_PATH = './data/embeddings/MentalTest'\n",
    "MODELS_DIR = './trained_models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized utility functions are defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Core Utility Functions (Optimized)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes the 'text' column of a batch of examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "def extract_cls_embeddings(batch):\n",
    "    \"\"\"Extracts [CLS] token embeddings from a batch of tokenized inputs.\"\"\"\n",
    "    input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return {'cls_embedding': cls_embeddings}\n",
    "\n",
    "def transform_to_broad_category_single(original_labels):\n",
    "    \"\"\"\n",
    "    Transforms a batch of multi-hot labels into a single-dimension array representing the dominant emotion.\n",
    "    PRECEDENCE RULE: Negative (0) > Positive (2) > Neutral (1).\n",
    "    \"\"\"\n",
    "    broad_labels = np.full(original_labels.shape[0], BROAD_EMOTION_CATEGORIES_MAPPING['neutral'], dtype=int)\n",
    "    has_positive = original_labels[:, list(positive_indices)].any(axis=1)\n",
    "    has_negative = original_labels[:, list(negative_indices)].any(axis=1)\n",
    "    broad_labels[has_positive] = BROAD_EMOTION_CATEGORIES_MAPPING['positive']\n",
    "    broad_labels[has_negative] = BROAD_EMOTION_CATEGORIES_MAPPING['negative']\n",
    "    return broad_labels\n",
    "\n",
    "print(\"Optimized utility functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Found pre-computed embeddings locally. Loading from disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Ingestion Complete ---\n",
      "Final training pool embeddings dataset: Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'attention_mask', 'cls_embedding'],\n",
      "    num_rows: 168980\n",
      "})\n",
      "Final test embeddings dataset: Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'attention_mask', 'cls_embedding'],\n",
      "    num_rows: 42245\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Ingestion and Feature Extraction Pipeline (NEW LOGIC)\n",
    "\n",
    "# This cell now automates the download of the pre-computed embeddings from Kaggle.\n",
    "# It checks if the data exists locally, and if not, uses the Kaggle CLI to download it.\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_SLUG = \"kianhutchinson/mentalheathdatabase\"\n",
    "LOCAL_DATA_PATH = \"./data/embeddings/\"\n",
    "\n",
    "train_pool_embeddings_dataset = None\n",
    "test_embeddings_dataset = None\n",
    "\n",
    "# --- Phase 1: Check if embeddings already exist locally --- \n",
    "if os.path.exists(EMBEDDINGS_TRAIN_POOL_PATH) and os.path.exists(EMBEDDINGS_TEST_PATH):\n",
    "    print(f\"Phase 1: Found pre-computed embeddings locally. Loading from disk.\")\n",
    "    try:\n",
    "        train_pool_embeddings_dataset = Dataset.load_from_disk(EMBEDDINGS_TRAIN_POOL_PATH)\n",
    "        test_embeddings_dataset = Dataset.load_from_disk(EMBEDDINGS_TEST_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading local embeddings: {e}. Attempting to re-download.\")\n",
    "        train_pool_embeddings_dataset = None\n",
    "\n",
    "# --- Phase 2: If embeddings not found, download from Kaggle --- \n",
    "if train_pool_embeddings_dataset is None:\n",
    "    print(f\"Phase 2: Local embeddings not found. Attempting to download from Kaggle: {DATASET_SLUG}\")\n",
    "    os.makedirs(LOCAL_DATA_PATH, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Construct and execute the Kaggle CLI command\n",
    "        command = [\n",
    "            \"kaggle\", \"datasets\", \"download\",\n",
    "            \"-d\", DATASET_SLUG,\n",
    "            \"-p\", LOCAL_DATA_PATH,\n",
    "            \"--unzip\"\n",
    "        ]\n",
    "        print(f\"\\nExecuting Kaggle command: {' '.join(command)}\")\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(\"\\n--- Kaggle CLI Output ---\")\n",
    "        print(result.stdout)\n",
    "        print(f\"Successfully downloaded and unzipped dataset to: {LOCAL_DATA_PATH}\")\n",
    "        \n",
    "        # Now that it's downloaded, load it from the disk\n",
    "        print(\"\\nLoading the newly downloaded embeddings...\")\n",
    "        train_pool_embeddings_dataset = Dataset.load_from_disk(EMBEDDINGS_TRAIN_POOL_PATH)\n",
    "        test_embeddings_dataset = Dataset.load_from_disk(EMBEDDINGS_TEST_PATH)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n--- DOWNLOAD FAILED ---\")\n",
    "        print(\"Error: The 'kaggle' command was not found. Please ensure the Kaggle library is installed ('pip install kaggle').\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"\\n--- DOWNLOAD FAILED ---\")\n",
    "        print(\"The Kaggle command failed to execute.\")\n",
    "        print(f\"Error details: {e.stderr}\")\n",
    "        print(\"Please ensure your Kaggle API token ('kaggle.json') is correctly configured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- AN UNEXPECTED ERROR OCCURRED ---\")\n",
    "        print(f\"Error details: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Ingestion Complete ---\")\n",
    "if train_pool_embeddings_dataset and test_embeddings_dataset:\n",
    "    print(\"Final training pool embeddings dataset:\", train_pool_embeddings_dataset)\n",
    "    print(\"Final test embeddings dataset:\", test_embeddings_dataset)\n",
    "else:\n",
    "    print(\"FATAL: Could not load or download the required embedding datasets. Please check the errors above.\")\n",
    "\n",
    "# --- Final Memory Cleanup --- \n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for scikit-learn models...\n",
      "Splitting the training pool into final train and validation sets...\n",
      "  - Final Training Set size: 152082\n",
      "  - Final Validation Set size: 16898\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation data...\n",
      "Processing test data...\n",
      "Cleaning up intermediate data objects to free memory...\n",
      "\n",
      "Class distribution of the training set before oversampling:\n",
      "2    63295\n",
      "1    49190\n",
      "0    39597\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution of the training set after oversampling:\n",
      "2    63295\n",
      "0    63295\n",
      "1    63295\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final shape of X_train (oversampled): (189885, 768)\n",
      "Final shape of Y_train (oversampled): (189885,)\n",
      "Final shape of X_val: (16898, 768)\n",
      "Final shape of Y_val: (16898,)\n",
      "Final shape of X_test: (42245, 768)\n",
      "Final shape of Y_test: (42245,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Data Preparation for Scikit-learn Models (MEMORY OPTIMIZED)\n",
    "\n",
    "# This cell has been re-engineered to be more memory-efficient and prevent kernel crashes.\n",
    "# It processes data sequentially and deletes large intermediate objects as soon as possible.\n",
    "\n",
    "print(\"\\nPreparing data for scikit-learn models...\")\n",
    "\n",
    "# --- Step 1: Train/Validation Split ---\n",
    "print(\"Splitting the training pool into final train and validation sets...\")\n",
    "train_val_split = train_pool_embeddings_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "validation_dataset = train_val_split['test']\n",
    "\n",
    "print(f\"  - Final Training Set size: {len(train_dataset)}\")\n",
    "print(f\"  - Final Validation Set size: {len(validation_dataset)}\")\n",
    "\n",
    "# --- Step 2: Process and Convert Data Sequentially for Memory Efficiency ---\n",
    "\n",
    "# Process Training Data\n",
    "print(\"Processing training data...\")\n",
    "X_train_raw = np.array(train_dataset['cls_embedding'], dtype='float32')\n",
    "Y_train_original_2D = np.array(train_dataset['labels'], dtype=int)\n",
    "Y_train_broad = transform_to_broad_category_single(Y_train_original_2D)\n",
    "\n",
    "# Process Validation Data\n",
    "print(\"Processing validation data...\")\n",
    "X_val = np.array(validation_dataset['cls_embedding'], dtype='float32')\n",
    "Y_val_original_2D = np.array(validation_dataset['labels'], dtype=int)\n",
    "Y_val = transform_to_broad_category_single(Y_val_original_2D)\n",
    "\n",
    "# Process Test Data\n",
    "print(\"Processing test data...\")\n",
    "X_test = np.array(test_embeddings_dataset['cls_embedding'], dtype='float32')\n",
    "Y_test_original_2D = np.array(test_embeddings_dataset['labels'], dtype=int)\n",
    "Y_test = transform_to_broad_category_single(Y_test_original_2D)\n",
    "\n",
    "# --- Step 3: Aggressive Memory Cleanup --- \n",
    "# Delete all large, intermediate objects before the memory-intensive oversampling step.\n",
    "print(\"Cleaning up intermediate data objects to free memory...\")\n",
    "del train_pool_embeddings_dataset, test_embeddings_dataset, train_dataset, validation_dataset\n",
    "del Y_train_original_2D, Y_val_original_2D, Y_test_original_2D\n",
    "gc.collect()\n",
    "\n",
    "# --- Step 4: Oversampling (Applied ONLY to the training set) ---\n",
    "print(\"\\nClass distribution of the training set before oversampling:\")\n",
    "print(pd.Series(Y_train_broad).value_counts())\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train, Y_train = ros.fit_resample(X_train_raw, Y_train_broad)\n",
    "\n",
    "print(\"\\nClass distribution of the training set after oversampling:\")\n",
    "print(pd.Series(Y_train).value_counts())\n",
    "\n",
    "# --- Final Data Shapes ---\n",
    "print(f\"\\nFinal shape of X_train (oversampled): {X_train.shape}\")\n",
    "print(f\"Final shape of Y_train (oversampled): {Y_train.shape}\")\n",
    "print(f\"Final shape of X_val: {X_val.shape}\")\n",
    "print(f\"Final shape of Y_val: {Y_val.shape}\")\n",
    "print(f\"Final shape of X_test: {X_test.shape}\")\n",
    "print(f\"Final shape of Y_test: {Y_test.shape}\")\n",
    "\n",
    "def train_save_evaluate_model(name, model_obj, X_train_data, Y_train_data, X_val_data, Y_val_data):\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    model_obj.fit(X_train_data, Y_train_data)\n",
    "    Y_val_pred = model_obj.predict(X_val_data)\n",
    "    micro_f1 = f1_score(Y_val_data, Y_val_pred, average='micro')\n",
    "    filename = os.path.join(MODELS_DIR, f\"{name.replace(' ', '_')}_MicroF1_{micro_f1:.4f}.pkl\")\n",
    "    joblib.dump(model_obj, filename)\n",
    "    print(f\"{name} Validation Metrics:\")\n",
    "    print(classification_report(Y_val_data, Y_val_pred, target_names=BROAD_EMOTION_CATEGORIES))\n",
    "    print(f\"Model saved to: {filename}\")\n",
    "    gc.collect()\n",
    "    return model_obj, Y_val_pred\n",
    "\n",
    "# Dictionaries to store trained models and their validation predictions\n",
    "models = {}\n",
    "val_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.66      0.59      4272\n",
      "     neutral       0.54      0.54      0.54      5515\n",
      "    positive       0.73      0.62      0.67      7111\n",
      "\n",
      "    accuracy                           0.60     16898\n",
      "   macro avg       0.60      0.61      0.60     16898\n",
      "weighted avg       0.62      0.60      0.61     16898\n",
      "\n",
      "Model saved to: ./trained_models/Logistic_Regression_MicroF1_0.6026.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Model Training - Logistic Regression\n",
    "\n",
    "models['Logistic Regression'], val_preds['Logistic Regression'] = train_save_evaluate_model(\n",
    "    'Logistic Regression',\n",
    "    LogisticRegression(max_iter=2000, solver='saga', C=0.2, n_jobs=-1, random_state=42),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Linear SVM ---\n",
      "Linear SVM Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.68      0.59      4272\n",
      "     neutral       0.54      0.52      0.53      5515\n",
      "    positive       0.73      0.62      0.67      7111\n",
      "\n",
      "    accuracy                           0.60     16898\n",
      "   macro avg       0.60      0.60      0.60     16898\n",
      "weighted avg       0.61      0.60      0.60     16898\n",
      "\n",
      "Model saved to: ./trained_models/Linear_SVM_MicroF1_0.6005.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Model Training - Linear SVM\n",
    "\n",
    "models['Linear SVM'], val_preds['Linear SVM'] = train_save_evaluate_model(\n",
    "    'Linear SVM',\n",
    "    LinearSVC(dual=False, max_iter=2000, C=0.1, random_state=42),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost ---\n",
      "XGBoost Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.67      0.61      4272\n",
      "     neutral       0.55      0.55      0.55      5515\n",
      "    positive       0.74      0.65      0.69      7111\n",
      "\n",
      "    accuracy                           0.62     16898\n",
      "   macro avg       0.61      0.62      0.62     16898\n",
      "weighted avg       0.63      0.62      0.62     16898\n",
      "\n",
      "Model saved to: ./trained_models/XGBoost_MicroF1_0.6205.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.3: Model Training - XGBoost\n",
    "\n",
    "models['XGBoost'], val_preds['XGBoost'] = train_save_evaluate_model(\n",
    "    'XGBoost',\n",
    "    XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth=5, objective='multi:softmax', \n",
    "                  eval_metric='mlogloss', use_label_encoder=False, tree_method='hist', n_jobs=-1, random_state=42),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.871324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 189885, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "LightGBM Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.67      0.61      4272\n",
      "     neutral       0.55      0.55      0.55      5515\n",
      "    positive       0.74      0.65      0.69      7111\n",
      "\n",
      "    accuracy                           0.62     16898\n",
      "   macro avg       0.62      0.62      0.62     16898\n",
      "weighted avg       0.63      0.62      0.63     16898\n",
      "\n",
      "Model saved to: ./trained_models/LightGBM_MicroF1_0.6240.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4: Model Training - LightGBM\n",
    "\n",
    "models['LightGBM'], val_preds['LightGBM'] = train_save_evaluate_model(\n",
    "    'LightGBM',\n",
    "    LGBMClassifier(n_estimators=1000, learning_rate=0.05, num_leaves=31, objective='softmax', \n",
    "                   n_jobs=-1, random_state=42),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Random Forest ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.65      0.60      4272\n",
      "     neutral       0.55      0.54      0.55      5515\n",
      "    positive       0.72      0.66      0.69      7111\n",
      "\n",
      "    accuracy                           0.62     16898\n",
      "   macro avg       0.61      0.62      0.61     16898\n",
      "weighted avg       0.63      0.62      0.62     16898\n",
      "\n",
      "Model saved to: ./trained_models/Random_Forest_MicroF1_0.6192.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: Model Training - Random Forest\n",
    "\n",
    "models['Random Forest'], val_preds['Random Forest'] = train_save_evaluate_model(\n",
    "    'Random Forest',\n",
    "    RandomForestClassifier(n_estimators=1500, max_depth=15, min_samples_leaf=3, n_jobs=-1, random_state=42),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CatBoost ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.67      0.60      4272\n",
      "     neutral       0.55      0.55      0.55      5515\n",
      "    positive       0.74      0.64      0.68      7111\n",
      "\n",
      "    accuracy                           0.62     16898\n",
      "   macro avg       0.61      0.62      0.61     16898\n",
      "weighted avg       0.63      0.62      0.62     16898\n",
      "\n",
      "Model saved to: ./trained_models/CatBoost_MicroF1_0.6175.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.6: Model Training - CatBoost\n",
    "\n",
    "models['CatBoost'], val_preds['CatBoost'] = train_save_evaluate_model(\n",
    "    'CatBoost',\n",
    "    CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=8, loss_function='MultiClass', \n",
    "                       verbose=0, random_seed=42, thread_count=-1),\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading All Saved Models for Final Evaluation ---\n",
      "  Successfully loaded model: 'XGBoost_MicroF1_0.6205.pkl'\n",
      "  Successfully loaded model: 'CatBoost_MicroF1_0.6175.pkl'\n",
      "  Successfully loaded model: 'Random_Forest_MicroF1_0.6192.pkl'\n",
      "  Successfully loaded model: 'Linear_SVM_MicroF1_0.6005.pkl'\n",
      "  Successfully loaded model: 'LightGBM_MicroF1_0.6240.pkl'\n",
      "  Successfully loaded model: 'Logistic_Regression_MicroF1_0.6026.pkl'\n",
      "\n",
      "--- Evaluating All Models on the Final Test Set ---\n",
      "\n",
      "--- Final Evaluation for: XGBoost_MicroF1_0.6205.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.67      0.60     10929\n",
      "     neutral       0.55      0.55      0.55     13794\n",
      "    positive       0.73      0.64      0.68     17522\n",
      "\n",
      "    accuracy                           0.62     42245\n",
      "   macro avg       0.61      0.62      0.61     42245\n",
      "weighted avg       0.63      0.62      0.62     42245\n",
      "\n",
      "\n",
      "--- Final Evaluation for: CatBoost_MicroF1_0.6175.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.66      0.60     10929\n",
      "     neutral       0.54      0.55      0.55     13794\n",
      "    positive       0.73      0.63      0.67     17522\n",
      "\n",
      "    accuracy                           0.61     42245\n",
      "   macro avg       0.61      0.61      0.61     42245\n",
      "weighted avg       0.62      0.61      0.61     42245\n",
      "\n",
      "\n",
      "--- Final Evaluation for: Random_Forest_MicroF1_0.6192.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.65      0.60     10929\n",
      "     neutral       0.55      0.54      0.54     13794\n",
      "    positive       0.72      0.66      0.69     17522\n",
      "\n",
      "    accuracy                           0.62     42245\n",
      "   macro avg       0.61      0.61      0.61     42245\n",
      "weighted avg       0.62      0.62      0.62     42245\n",
      "\n",
      "\n",
      "--- Final Evaluation for: Linear_SVM_MicroF1_0.6005.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.67      0.58     10929\n",
      "     neutral       0.54      0.52      0.53     13794\n",
      "    positive       0.72      0.61      0.66     17522\n",
      "\n",
      "    accuracy                           0.60     42245\n",
      "   macro avg       0.59      0.60      0.59     42245\n",
      "weighted avg       0.61      0.60      0.60     42245\n",
      "\n",
      "\n",
      "--- Final Evaluation for: LightGBM_MicroF1_0.6240.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.66      0.61     10929\n",
      "     neutral       0.55      0.55      0.55     13794\n",
      "    positive       0.73      0.65      0.69     17522\n",
      "\n",
      "    accuracy                           0.62     42245\n",
      "   macro avg       0.62      0.62      0.62     42245\n",
      "weighted avg       0.63      0.62      0.62     42245\n",
      "\n",
      "\n",
      "--- Final Evaluation for: Logistic_Regression_MicroF1_0.6026.pkl ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.65      0.58     10929\n",
      "     neutral       0.53      0.53      0.53     13794\n",
      "    positive       0.72      0.61      0.66     17522\n",
      "\n",
      "    accuracy                           0.60     42245\n",
      "   macro avg       0.59      0.60      0.59     42245\n",
      "weighted avg       0.61      0.60      0.60     42245\n",
      "\n",
      "\n",
      "===========================================================================\n",
      "             FINAL MODEL PERFORMANCE LEADERBOARD (ON TEST SET)             \n",
      "===========================================================================\n",
      "  - Model: LightGBM_MicroF1_0.6240.pkl                        | F1 Score: 0.6223 | Accuracy: 0.6223\n",
      "  - Model: XGBoost_MicroF1_0.6205.pkl                         | F1 Score: 0.6167 | Accuracy: 0.6167\n",
      "  - Model: Random_Forest_MicroF1_0.6192.pkl                   | F1 Score: 0.6160 | Accuracy: 0.6160\n",
      "  - Model: CatBoost_MicroF1_0.6175.pkl                        | F1 Score: 0.6108 | Accuracy: 0.6108\n",
      "  - Model: Linear_SVM_MicroF1_0.6005.pkl                      | F1 Score: 0.5973 | Accuracy: 0.5973\n",
      "  - Model: Logistic_Regression_MicroF1_0.6026.pkl             | F1 Score: 0.5960 | Accuracy: 0.5960\n",
      "\n",
      "***************************************************************************\n",
      "             OVERALL BEST MODEL: \"LightGBM_MicroF1_0.6240.pkl\"             \n",
      "                (With F1 Score: 0.6223 and Accuracy: 0.6223)               \n",
      "***************************************************************************\n",
      "\n",
      "Successfully loaded champion model 'LightGBM_MicroF1_0.6240.pkl' for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Definitive Evaluation and Champion Selection (UPDATED)\n",
    "\n",
    "# This cell is now a standalone evaluation script. It ignores any models in memory\n",
    "# and instead loads ALL .pkl files from the 'trained_models' directory for a comprehensive test.\n",
    "\n",
    "print(\"\\n--- Loading All Saved Models for Final Evaluation ---\")\n",
    "\n",
    "# Use a new dictionary to hold the models loaded from disk\n",
    "loaded_models = {}\n",
    "\n",
    "if os.path.exists(MODELS_DIR):\n",
    "    for model_filename in os.listdir(MODELS_DIR):\n",
    "        if model_filename.endswith(\".pkl\"):\n",
    "            try:\n",
    "                model_path = os.path.join(MODELS_DIR, model_filename)\n",
    "                # The key is the full filename for explicit identification\n",
    "                loaded_models[model_filename] = joblib.load(model_path)\n",
    "                print(f\"  Successfully loaded model: '{model_filename}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Failed to load {model_filename} due to: {e}\")\n",
    "else:\n",
    "    print(f\"Warning: Models directory '{MODELS_DIR}' not found. Cannot load any models.\")\n",
    "\n",
    "# Proceed only if models were successfully loaded.\n",
    "if not loaded_models:\n",
    "    print(\"\\nFATAL: No models available to evaluate. Please run the training cells first.\")\n",
    "else:\n",
    "    print(\"\\n--- Evaluating All Models on the Final Test Set ---\")\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    for name, model_instance in loaded_models.items():\n",
    "        print(f\"\\n--- Final Evaluation for: {name} ---\")\n",
    "        Y_test_pred = model_instance.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "        micro_f1 = f1_score(Y_test, Y_test_pred, average='micro')\n",
    "        test_results[name] = {'accuracy': accuracy, 'micro_f1': micro_f1}\n",
    "        \n",
    "        print(classification_report(Y_test, Y_test_pred, target_names=BROAD_EMOTION_CATEGORIES, zero_division=0))\n",
    "    \n",
    "    # Determine the best model and print the final summary list.\n",
    "    if test_results:\n",
    "        final_summary = sorted(test_results.items(), key=lambda item: item[1]['micro_f1'], reverse=True)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*75)\n",
    "        print(\"FINAL MODEL PERFORMANCE LEADERBOARD (ON TEST SET)\".center(75))\n",
    "        print(\"=\"*75)\n",
    "        \n",
    "        for model_name, scores in final_summary:\n",
    "            print(f\"  - Model: {model_name:<50} | F1 Score: {scores['micro_f1']:.4f} | Accuracy: {scores['accuracy']:.4f}\")\n",
    "        \n",
    "        # Announce the best performing model\n",
    "        best_model_name, best_model_scores = final_summary[0]\n",
    "        \n",
    "        print(\"\\n\" + \"*\"*75)\n",
    "        print(f\"OVERALL BEST MODEL: \\\"{best_model_name}\\\"\".center(75))\n",
    "        print(f\"(With F1 Score: {best_model_scores['micro_f1']:.4f} and Accuracy: {best_model_scores['accuracy']:.4f})\" .center(75))\n",
    "        print(\"*\"*75)\n",
    "\n",
    "        # Set the final model for the inference cell, using the winner from the loaded models\n",
    "        final_model = loaded_models[best_model_name]\n",
    "        print(f\"\\nSuccessfully loaded champion model '{best_model_name}' for prediction.\")\n",
    "    else:\n",
    "        print(\"\\nEvaluation could not be completed as no models were successfully tested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prediction function is ready.\n",
      "\n",
      "--- Demonstrating Predictions with the Best Model: LightGBM_MicroF1_0.6240.pkl ---\n",
      "\n",
      "Text: \"I am absolutely thrilled with the results, feeling pure joy and excitement!\"\n",
      "--> Predicted Emotion: positive (index: 2)\n",
      "\n",
      "Text: \"This is quite disappointing, I had hoped for a better outcome, feeling a bit sad.\"\n",
      "--> Predicted Emotion: negative (index: 0)\n",
      "\n",
      "Text: \"The instructions for this assembly are confusing, I'm just utterly lost.\"\n",
      "--> Predicted Emotion: negative (index: 0)\n",
      "\n",
      "Text: \"I feel nothing. Completely devoid of any emotion.\"\n",
      "--> Predicted Emotion: neutral (index: 1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Prediction Function for New Text Data (Optimized)\n",
    "\n",
    "# This function now uses the 'final_model' object that was loaded into memory in the previous cell.\n",
    "# This is far more efficient than loading the model from disk on every single call.\n",
    "\n",
    "def predict_emotion(text: str):\n",
    "    \"\"\"\n",
    "    Predicts a single broad emotion category for a given text using the best trained model.\n",
    "    \"\"\"\n",
    "    # Ensure the global 'final_model' has been loaded\n",
    "    if 'final_model' not in globals():\n",
    "        raise RuntimeError(\"The 'final_model' is not loaded. Please run the evaluation cell (Cell 6) first.\")\n",
    "\n",
    "    # The main Transformer model needs to be in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text and move to the correct device\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get the embedding from the Transformer model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "    # Use the pre-loaded champion classifier to predict\n",
    "    predicted_class_index = final_model.predict(text_embedding)[0]\n",
    "    predicted_emotion_name = BROAD_EMOTION_CATEGORIES[predicted_class_index]\n",
    "\n",
    "    return predicted_emotion_name, predicted_class_index\n",
    "\n",
    "print(\"Optimized prediction function is ready.\")\n",
    "\n",
    "# --- Demonstration --- \n",
    "try:\n",
    "    print(f\"\\n--- Demonstrating Predictions with the Best Model: {best_model_name} ---\")\n",
    "\n",
    "    texts_to_predict = [\n",
    "        \"I am absolutely thrilled with the results, feeling pure joy and excitement!\",\n",
    "        \"This is quite disappointing, I had hoped for a better outcome, feeling a bit sad.\",\n",
    "        \"The instructions for this assembly are confusing, I'm just utterly lost.\",\n",
    "        \"I feel nothing. Completely devoid of any emotion.\"\n",
    "    ]\n",
    "\n",
    "    for text in texts_to_predict:\n",
    "        name, index = predict_emotion(text)\n",
    "        print(f'\\nText: \"{text}\"')\n",
    "        print(f'--> Predicted Emotion: {name} (index: {index})')\n",
    "except NameError:\n",
    "    print(\"\\nCould not run prediction examples because the 'final_model' was not set. Please run Cell 6 first.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "GPU T4 x2",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
